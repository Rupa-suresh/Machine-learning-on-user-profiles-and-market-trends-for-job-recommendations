{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CQ0WdnvZv2d"
      },
      "source": [
        "loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoT4NkchUtH6",
        "outputId": "f0b7b5b1-655d-404d-e921-d1e3dca88075"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'scraped_data (1).csv'\n",
        "df = pd.read_csv(file_path)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "# Display basic info\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPjy9fNlZ2Ge"
      },
      "source": [
        "basic data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsfhelzTUynX",
        "outputId": "a9f070d2-d383-4538-d29e-99868eb1fab3"
      },
      "outputs": [],
      "source": [
        "# Remove duplicates based on all columns\n",
        "df = df.drop_duplicates()\n",
        "print(\"Shape after removing duplicates:\", df.shape)\n",
        "\n",
        "# Check for duplicates based on job_link (unique identifier)\n",
        "df = df.drop_duplicates(subset=[\"job_link\"], keep=\"first\")\n",
        "print(\"Shape after removing duplicates by job_link:\", df.shape)\n",
        "\n",
        "# Confirm no missing values remain (should be zero already)\n",
        "print(\"Missing values after cleaning:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAA8XPVaBy6"
      },
      "source": [
        "data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzlWk7GUaEMI"
      },
      "source": [
        "process tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOKZQqUqU2Xk",
        "outputId": "889dc5e2-382f-499d-a3b2-2700566da96a"
      },
      "outputs": [],
      "source": [
        "# 3.1 Process Tags\n",
        "df[\"Tags\"] = df[\"Tags\"].str.lower().str.replace(\", \", \",\").str.strip()\n",
        "print(\"Sample Tags after cleaning:\")\n",
        "print(df[\"Tags\"].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnsMQmaFaGrn"
      },
      "source": [
        "parse and normalize salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WO-ZHw9U5y0",
        "outputId": "08e85b5a-0655-48cd-89d7-9c9044d2d3f3"
      },
      "outputs": [],
      "source": [
        "# 3.2 Parse and Normalize Salary (Revised)\n",
        "def parse_salary(salary):\n",
        "    if \"not disclosed\" in salary.lower() or pd.isna(salary):\n",
        "        return None\n",
        "    salary = salary.lower().replace(\"lacs pa\", \"\").replace(\"lpa\", \"\").replace(\"(including variable: 15.0%)\", \"\").replace(\",\", \"\").strip()\n",
        "    if \"pa\" in salary:  # Assume rupees annually, convert to LPA\n",
        "        try:\n",
        "            value = float(salary.replace(\"pa\", \"\").strip()) / 100000  # e.g., \"50000 pa\" â†’ 0.5 LPA\n",
        "            return value if value < 100 else None  # Cap at 100 LPA to catch errors\n",
        "        except ValueError:\n",
        "            return None\n",
        "    if \"-\" in salary:\n",
        "        low, high = map(float, salary.split(\"-\"))\n",
        "        return (low + high) / 2 if (low < 100 and high < 100) else None  # Cap at 100 LPA\n",
        "    try:\n",
        "        value = float(salary)\n",
        "        return value if value < 100 else None  # Cap at 100 LPA\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "df[\"salary_clean\"] = df[\"Salary\"].apply(parse_salary)\n",
        "mean_salary = df[\"salary_clean\"].mean()\n",
        "print(\"Mean salary before imputation:\", mean_salary)\n",
        "df[\"salary_clean\"] = df[\"salary_clean\"].fillna(mean_salary)  \n",
        "df[\"salary_norm\"] = df[\"salary_clean\"] / df[\"salary_clean\"].max()\n",
        "print(\"\\nSalary Stats:\")\n",
        "print(df[\"salary_clean\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7eVeLDjacvt"
      },
      "source": [
        "parse and normalize experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1ylZJhEU9os",
        "outputId": "6e8ff0be-f88d-4231-d6f7-b3c785442f35"
      },
      "outputs": [],
      "source": [
        "# 3.3 Parse and Normalize Experience\n",
        "def parse_experience(exp):\n",
        "    exp = exp.lower().replace(\"yrs\", \"\").strip()\n",
        "    if \"-\" in exp:\n",
        "        low, high = map(int, exp.split(\"-\"))\n",
        "        return (low + high) / 2\n",
        "    return float(exp.split()[0]) if exp else 0\n",
        "\n",
        "df[\"exp_clean\"] = df[\"Experience\"].apply(parse_experience)\n",
        "mean_exp = df[\"exp_clean\"].mean()\n",
        "print(\"Mean experience before imputation:\", mean_exp)\n",
        "df[\"exp_clean\"] = df[\"exp_clean\"].fillna(mean_exp)  \n",
        "df[\"exp_norm\"] = df[\"exp_clean\"] / df[\"exp_clean\"].max()\n",
        "print(\"\\nExperience Stats:\")\n",
        "print(df[\"exp_clean\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEmooR7VaiJt"
      },
      "source": [
        "encode location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwbYXyYMVKKh",
        "outputId": "bbcd4a6b-2e9c-4871-8eae-5b6f579fde95"
      },
      "outputs": [],
      "source": [
        "# 3.4 Encode Location\n",
        "df[\"location_clean\"] = df[\"Location\"].str.replace(\"Hybrid - \", \"\").str.split(\",\").str[0].str.strip().str.lower()\n",
        "top_locations = df[\"location_clean\"].value_counts().index[:10]\n",
        "for loc in top_locations:\n",
        "    df[f\"loc_{loc}\"] = df[\"location_clean\"].apply(lambda x: 1 if x == loc else 0)\n",
        "print(\"\\nTop 10 Locations:\")\n",
        "print(df[\"location_clean\"].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Tl3b7ibAvW"
      },
      "source": [
        "feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-NkhUIaVP0c",
        "outputId": "54f50759-abf2-41d9-989a-190589f0ce27"
      },
      "outputs": [],
      "source": [
        "# 3.5 Create Feature Matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# TF-IDF for Tags\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\",\"))\n",
        "skill_matrix = vectorizer.fit_transform(df[\"Tags\"]).toarray()\n",
        "print(\"Skill Matrix Shape:\", skill_matrix.shape)\n",
        "\n",
        "# Numerical features\n",
        "num_features = df[[\"salary_norm\", \"exp_norm\"]].values\n",
        "print(\"Numerical Features Shape:\", num_features.shape)\n",
        "\n",
        "# Location features (top 10)\n",
        "top_locations = df[\"location_clean\"].value_counts().index[:10]  \n",
        "loc_features = df[[f\"loc_{loc}\" for loc in top_locations]].values\n",
        "print(\"Location Features Shape:\", loc_features.shape)\n",
        "\n",
        "# Combine into feature matrix\n",
        "job_features = np.hstack((skill_matrix, num_features, loc_features))\n",
        "print(\"\\nFinal Feature Matrix Shape:\", job_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2pXoNL_VWsU",
        "outputId": "037d2d70-fb6f-46a9-9d26-edb7fe82a9a9"
      },
      "outputs": [],
      "source": [
        "# Save cleaned DataFrame for later use\n",
        "df.to_csv('scrapedjobs_cleaned.csv', index=False)\n",
        "print(\"Cleaned dataset saved to Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAJ170IJbDZl"
      },
      "source": [
        "clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyL2O4OXbF5m"
      },
      "source": [
        "test kmeans for different k values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4EAFCdDVZ3P",
        "outputId": "534c33d2-7756-408a-a30b-94ac8242570c"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Test k values from 5 to 20\n",
        "k_values = range(5, 21)\n",
        "silhouette_scores = []\n",
        "inertia_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(job_features)\n",
        "    silhouette = silhouette_score(job_features, clusters)\n",
        "    inertia = kmeans.inertia_\n",
        "    silhouette_scores.append(silhouette)\n",
        "    inertia_scores.append(inertia)\n",
        "    print(f\"k={k}, Silhouette Score={silhouette:.3f}, Inertia={inertia:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg3wi28NbMst"
      },
      "source": [
        "clustering with dimensionality reduction and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnlzzNHMbSPG"
      },
      "source": [
        "reduce dimensionality with truncated SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWCGounvVfmt",
        "outputId": "48b79f26-f175-44d4-bd8e-c1c963e7cc59"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "\n",
        "# Reduce dimensions to 50 components\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "job_features_reduced = svd.fit_transform(job_features)\n",
        "print(\"Reduced Feature Matrix Shape:\", job_features_reduced.shape)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(f\"Explained Variance Ratio (50 components): {explained_variance:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_YhTTJzbWym"
      },
      "source": [
        "test kmeans on reduced features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9ec3Be5WOrf",
        "outputId": "1320781b-121c-4ae9-a30e-420a9b331409"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Test k values from 5 to 20\n",
        "k_values = range(5, 21)\n",
        "silhouette_scores = []\n",
        "inertia_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(job_features_reduced)\n",
        "    silhouette = silhouette_score(job_features_reduced, clusters)\n",
        "    inertia = kmeans.inertia_\n",
        "    silhouette_scores.append(silhouette)\n",
        "    inertia_scores.append(inertia)\n",
        "    print(f\"k={k}, Silhouette Score={silhouette:.3f}, Inertia={inertia:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4wYTTmEbs8V"
      },
      "source": [
        "analyze and pick best k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "hfpohPtYWTWd",
        "outputId": "4352f20b-344e-48c9-dd6f-639f9bd51170"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Find best k\n",
        "best_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
        "print(f\"\\nBest k: {best_k}, Silhouette Score: {max(silhouette_scores):.3f}\")\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score vs. k (Reduced Features)')\n",
        "plt.show()\n",
        "\n",
        "# Plot inertia\n",
        "plt.plot(k_values, inertia_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Inertia vs. k (Reduced Features)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n90xNWD8by0p"
      },
      "source": [
        "apply final clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tebbv8DgWbUW",
        "outputId": "024895b6-fd6d-4d66-97bb-63165233aea0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "df[\"cluster\"] = kmeans.fit_predict(job_features_reduced)\n",
        "print(\"Clustering complete. Added 'cluster' column to DataFrame.\")\n",
        "print(\"Cluster distribution:\")\n",
        "print(df[\"cluster\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWa_DVXUb7dj"
      },
      "source": [
        "process userinput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZbtBTh5Wfh5",
        "outputId": "1d301220-f56a-4c1d-a3a0-0ae7e08eacc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_user_input(skills, salary, exp, location):\n",
        "    # Skills to TF-IDF\n",
        "    skills_clean = skills.lower().replace(\", \", \",\").strip()\n",
        "    skills_vec = vectorizer.transform([skills_clean]).toarray() \n",
        "\n",
        "    # Salary (assume LPA)\n",
        "    salary_val = float(salary) if salary else df[\"salary_clean\"].mean()\n",
        "    salary_norm = salary_val / df[\"salary_clean\"].max()\n",
        "\n",
        "    # Experience\n",
        "    exp_val = float(exp) if exp else df[\"exp_clean\"].mean()\n",
        "    exp_norm = exp_val / df[\"exp_clean\"].max()\n",
        "\n",
        "    # Location (one-hot for top 10)\n",
        "    loc_clean = location.lower().strip()\n",
        "    loc_vec = np.array([1 if loc_clean == loc else 0 for loc in top_locations]).reshape(1, -1)\n",
        "\n",
        "    # Combine features\n",
        "    user_features = np.hstack((skills_vec, [[salary_norm, exp_norm]], loc_vec))\n",
        "\n",
        "    # Reduce with SVD \n",
        "    user_features_reduced = svd.transform(user_features)\n",
        "    return user_features_reduced\n",
        "\n",
        "\n",
        "#  test with sample values \n",
        "sample_skills = \"python, sql, machine learning\"\n",
        "sample_salary = \"10\"  # LPA\n",
        "sample_exp = \"5\"      # years\n",
        "sample_location = \"bengaluru\"\n",
        "user_input = preprocess_user_input(sample_skills, sample_salary, sample_exp, sample_location)\n",
        "print(\"User Features Reduced Shape:\", user_input.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHlduNhKb_Ho"
      },
      "source": [
        "assign user to cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaqUhtI7WmXH",
        "outputId": "aaea71d1-9fda-44ff-d108-9f36145c0f7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.reset_index(drop=True)\n",
        "print(\"DataFrame index reset. New shape:\", df.shape)\n",
        "\n",
        "# Find user's cluster\n",
        "user_cluster = kmeans.predict(user_input)[0]\n",
        "print(f\"User assigned to Cluster: {user_cluster}\")\n",
        "\n",
        "# Filter jobs in user's cluster\n",
        "cluster_jobs_idx = df[df[\"cluster\"] == user_cluster].index\n",
        "cluster_job_features = job_features_reduced[cluster_jobs_idx]\n",
        "print(f\"Number of jobs in Cluster {user_cluster}: {len(cluster_jobs_idx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3SddoYccEHL"
      },
      "source": [
        "knn recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiuzYNhbWoEh",
        "outputId": "f095ac90-ad39-4a5c-89ce-16e579946339"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# kNN for top 5 recommendations\n",
        "knn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
        "knn.fit(cluster_job_features)\n",
        "distances, indices = knn.kneighbors(user_input)\n",
        "recommended_indices = cluster_jobs_idx[indices[0]]\n",
        "recommendations = df.iloc[recommended_indices][[\"Title\", \"Tags\", \"Salary\", \"Location\",\"Experience\",\"job_link\"]]\n",
        "print(\"\\nTop 5 Recommended Jobs:\")\n",
        "print(recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HGkYXIkcHOH"
      },
      "source": [
        "compute metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6yFoE16Ws55",
        "outputId": "86e713b3-a500-4351-fadf-9c4253329406"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Cosine similarity for recommendations\n",
        "similarities = 1 - distances[0]  # Convert distance to similarity\n",
        "avg_similarity = similarities.mean()\n",
        "print(f\"\\nAverage Cosine Similarity: {avg_similarity:.3f}\")\n",
        "\n",
        "# Baseline: Random jobs from cluster\n",
        "random_idx = np.random.choice(cluster_jobs_idx, 5, replace=False)\n",
        "random_features = job_features_reduced[random_idx]\n",
        "random_similarities = cosine_similarity(user_input, random_features)[0]\n",
        "avg_random_similarity = random_similarities.mean()\n",
        "print(f\"Random Baseline Similarity: {avg_random_similarity:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Loaded data with 3978 rows and 24 columns\n",
            "Processing tags...\n",
            "Creating TF-IDF vectorizer...\n",
            "Skill matrix shape: (3978, 5371)\n",
            "Saved vectorizer\n",
            "Preparing numerical features...\n",
            "Numerical features shape: (3978, 2)\n",
            "Preparing location features...\n",
            "Top 10 locations: ['mumbai', 'bengaluru', 'pune', 'hyderabad', 'gurugram', 'chennai', 'kolkata', 'coimbatore', 'noida', 'ahmedabad']\n",
            "Location features shape: (3978, 10)\n",
            "Creating combined feature matrix...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\praga\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job features shape: (3978, 5383)\n",
            "Saved feature information\n",
            "Creating SVD model...\n",
            "Reduced job features shape: (3978, 50)\n",
            "Explained variance ratio: 0.551\n",
            "Saved SVD model\n",
            "Creating K-means model...\n",
            "Cluster distribution:\n",
            "cluster\n",
            "2    1012\n",
            "0     853\n",
            "3     731\n",
            "4     581\n",
            "1     356\n",
            "6     168\n",
            "7     149\n",
            "5     128\n",
            "Name: count, dtype: int64\n",
            "Saved K-means model\n",
            "Saving processed job data...\n",
            "Saved processed job data\n",
            "Saving reduced feature matrix...\n",
            "Saved reduced feature matrix\n",
            "\n",
            "All models and data saved successfully in the 'models' directory!\n",
            "Files saved:\n",
            "- feature_info.pkl (0.00 MB)\n",
            "- job_features_reduced.npy (1.52 MB)\n",
            "- kmeans.pkl (0.02 MB)\n",
            "- processed_jobs.csv (1.82 MB)\n",
            "- svd.pkl (2.06 MB)\n",
            "- vectorizer.pkl (0.14 MB)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Create a function to replace the lambda\n",
        "def tokenize_tags(text):\n",
        "    return text.split(\",\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "# Load the cleaned data \n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('scrapedjobs_cleaned.csv')\n",
        "print(f\"Loaded data with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "\n",
        "# Process tags for TF-IDF\n",
        "print(\"Processing tags...\")\n",
        "df[\"Tags\"] = df[\"Tags\"].str.lower().str.replace(\", \", \",\").str.strip()\n",
        "\n",
        "# 1. Create and save the TF-IDF vectorizer\n",
        "print(\"Creating TF-IDF vectorizer...\")\n",
        "# Use the named function instead of lambda\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize_tags)\n",
        "skill_matrix = vectorizer.fit_transform(df[\"Tags\"]).toarray()\n",
        "print(f\"Skill matrix shape: {skill_matrix.shape}\")\n",
        "with open('models/vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "print(\"Saved vectorizer\")\n",
        "\n",
        "# 2. Prepare numerical features\n",
        "print(\"Preparing numerical features...\")\n",
        "salary_max = df[\"salary_clean\"].max()\n",
        "exp_max = df[\"exp_clean\"].max()\n",
        "salary_mean = df[\"salary_clean\"].mean()\n",
        "exp_mean = df[\"exp_clean\"].mean()\n",
        "\n",
        "df[\"salary_norm\"] = df[\"salary_clean\"] / salary_max\n",
        "df[\"exp_norm\"] = df[\"exp_clean\"] / exp_max\n",
        "num_features = df[[\"salary_norm\", \"exp_norm\"]].values\n",
        "print(f\"Numerical features shape: {num_features.shape}\")\n",
        "\n",
        "# 3. Prepare location features\n",
        "print(\"Preparing location features...\")\n",
        "top_locations = df[\"location_clean\"].value_counts().index[:10].tolist()\n",
        "print(f\"Top 10 locations: {top_locations}\")\n",
        "\n",
        "# Check if location columns exist, if not create them\n",
        "for loc in top_locations:\n",
        "    col_name = f\"loc_{loc}\"\n",
        "    if col_name not in df.columns:\n",
        "        df[col_name] = df[\"location_clean\"].apply(lambda x: 1 if x == loc else 0)\n",
        "\n",
        "loc_features = df[[f\"loc_{loc}\" for loc in top_locations]].values\n",
        "print(f\"Location features shape: {loc_features.shape}\")\n",
        "\n",
        "# 4. Combine into feature matrix\n",
        "print(\"Creating combined feature matrix...\")\n",
        "job_features = np.hstack((skill_matrix, num_features, loc_features))\n",
        "print(f\"Job features shape: {job_features.shape}\")\n",
        "\n",
        "# Save feature information\n",
        "with open('models/feature_info.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'top_locations': top_locations,\n",
        "        'salary_max': salary_max,\n",
        "        'exp_max': exp_max,\n",
        "        'salary_mean': salary_mean,\n",
        "        'exp_mean': exp_mean\n",
        "    }, f)\n",
        "print(\"Saved feature information\")\n",
        "\n",
        "# 5. Create and save SVD model\n",
        "print(\"Creating SVD model...\")\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "job_features_reduced = svd.fit_transform(job_features)\n",
        "print(f\"Reduced job features shape: {job_features_reduced.shape}\")\n",
        "print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.3f}\")\n",
        "with open('models/svd.pkl', 'wb') as f:\n",
        "    pickle.dump(svd, f)\n",
        "print(\"Saved SVD model\")\n",
        "\n",
        "# 6. Create and save K-means model with 8 clusters\n",
        "print(\"Creating K-means model...\")\n",
        "kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
        "df[\"cluster\"] = kmeans.fit_predict(job_features_reduced)\n",
        "print(\"Cluster distribution:\")\n",
        "print(df[\"cluster\"].value_counts())\n",
        "with open('models/kmeans.pkl', 'wb') as f:\n",
        "    pickle.dump(kmeans, f)\n",
        "print(\"Saved K-means model\")\n",
        "\n",
        "# 7. Save processed data\n",
        "print(\"Saving processed job data...\")\n",
        "df.to_csv('models/processed_jobs.csv', index=False)\n",
        "print(\"Saved processed job data\")\n",
        "\n",
        "# 8. Save job_features_reduced as it's needed for KNN\n",
        "print(\"Saving reduced feature matrix...\")\n",
        "np.save('models/job_features_reduced.npy', job_features_reduced)\n",
        "print(\"Saved reduced feature matrix\")\n",
        "\n",
        "print(\"\\nAll models and data saved successfully in the 'models' directory!\")\n",
        "print(\"Files saved:\")\n",
        "for file in os.listdir('models'):\n",
        "    file_size = os.path.getsize(f'models/{file}') / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"- {file} ({file_size:.2f} MB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Getting recommendations for:\n",
            "Skills: python, java , pyspark , \n",
            "Salary: 2 LPA\n",
            "Experience: 0 years\n",
            "Location: noida\n",
            "==================================================\n",
            "\n",
            "Loading models and data...\n",
            "Loaded processed jobs: 3978 rows\n",
            "Loaded vectorizer\n",
            "Loaded SVD model\n",
            "Loaded K-means model\n",
            "Loaded feature information\n",
            "Loaded reduced job features: (3978, 50)\n",
            "Preprocessing user input...\n",
            "Skills vector shape: (1, 5371)\n",
            "Normalized salary: 0.032\n",
            "Normalized experience: 0.000\n",
            "Location vector shape: (1, 10)\n",
            "Combined user features shape: (1, 5383)\n",
            "Reduced user features shape: (1, 50)\n",
            "Getting recommendations...\n",
            "User assigned to cluster: 4\n",
            "Number of jobs in cluster 4: 581\n",
            "Average similarity score: 0.956\n",
            "\n",
            "==================================================\n",
            "Recommendation Results:\n",
            "User assigned to cluster: 4\n",
            "Average similarity score: 0.956 (95.6%)\n",
            "Top 5 recommendations:\n",
            "==================================================\n",
            "\n",
            "Recommendation #1 (Similarity: 96.7%):\n",
            "Title: Azure Devops Engineer\n",
            "Company: Infogain\n",
            "Salary: Not disclosed\n",
            "Location: Noida, Pune, Mumbai (All Areas)\n",
            "Experience: 4-6 Yrs\n",
            "Job Link: https://www.naukri.com/job-listings-azure-devops-engineer-infogain-noida-pune-mumbai-all-areas-4-to-6-years-091224017008\n",
            "\n",
            "Recommendation #2 (Similarity: 95.8%):\n",
            "Title: Processor Front Office\n",
            "Company: Yes Bank\n",
            "Salary: Not disclosed\n",
            "Location: Noida\n",
            "Experience: 0-3 Yrs\n",
            "Job Link: https://www.naukri.com/job-listings-processor-front-office-yes-bank-ltd-noida-0-to-3-years-181224918269\n",
            "\n",
            "Recommendation #3 (Similarity: 95.5%):\n",
            "Title: Hiring For BTECH Graduates II 27K II HR Muskan\n",
            "Company: Ienergizer\n",
            "Salary: 1.5-2.25 Lacs PA\n",
            "Location: Noida\n",
            "Experience: 0-4 Yrs\n",
            "Job Link: https://www.naukri.com/job-listings-hiring-for-btech-graduates-ii-27k-ii-hr-muskan-ienergizer-noida-0-to-4-years-171224006430\n",
            "\n",
            "Recommendation #4 (Similarity: 95.1%):\n",
            "Title: Product Management Intern\n",
            "Company: Paytm\n",
            "Salary: Not disclosed\n",
            "Location: Noida\n",
            "Experience: 0-1 Yrs\n",
            "Job Link: https://www.naukri.com/job-listings-product-management-intern-paytm-noida-0-to-1-years-101224008642\n",
            "\n",
            "Recommendation #5 (Similarity: 94.9%):\n",
            "Title: Customer Solutions Architect\n",
            "Company: Nokia\n",
            "Salary: Not disclosed\n",
            "Location: Noida\n",
            "Experience: 2-7 Yrs\n",
            "Job Link: https://www.naukri.com/job-listings-customer-solutions-architect-nokia-solutions-and-networks-india-p-ltd-noida-2-to-7-years-171224915260\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Function for tokenizing in the same way as during model creation\n",
        "def tokenize_tags(text):\n",
        "    return text.split(\",\")\n",
        "\n",
        "# Load all the saved models and data\n",
        "def load_models():\n",
        "    print(\"Loading models and data...\")\n",
        "    \n",
        "    # Load processed job data\n",
        "    df = pd.read_csv('models/processed_jobs.csv')\n",
        "    print(f\"Loaded processed jobs: {df.shape[0]} rows\")\n",
        "    \n",
        "    # Load vectorizer\n",
        "    with open('models/vectorizer.pkl', 'rb') as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "    print(\"Loaded vectorizer\")\n",
        "    \n",
        "    # Load SVD model\n",
        "    with open('models/svd.pkl', 'rb') as f:\n",
        "        svd = pickle.load(f)\n",
        "    print(\"Loaded SVD model\")\n",
        "    \n",
        "    # Load K-means model\n",
        "    with open('models/kmeans.pkl', 'rb') as f:\n",
        "        kmeans = pickle.load(f)\n",
        "    print(\"Loaded K-means model\")\n",
        "    \n",
        "    # Load feature information\n",
        "    with open('models/feature_info.pkl', 'rb') as f:\n",
        "        feature_info = pickle.load(f)\n",
        "    print(\"Loaded feature information\")\n",
        "    \n",
        "    # Load reduced job features\n",
        "    job_features_reduced = np.load('models/job_features_reduced.npy')\n",
        "    print(f\"Loaded reduced job features: {job_features_reduced.shape}\")\n",
        "    \n",
        "    return df, vectorizer, svd, kmeans, feature_info, job_features_reduced\n",
        "\n",
        "# Preprocess user input to match model input format\n",
        "def preprocess_user_input(skills, salary, exp, location, vectorizer, svd, feature_info):\n",
        "    print(\"Preprocessing user input...\")\n",
        "    \n",
        "    # Skills to TF-IDF\n",
        "    skills_clean = skills.lower().replace(\", \", \",\").strip()\n",
        "    skills_vec = vectorizer.transform([skills_clean]).toarray()\n",
        "    print(f\"Skills vector shape: {skills_vec.shape}\")\n",
        "\n",
        "    # Salary (assume LPA)\n",
        "    if salary and salary.strip():\n",
        "        salary_val = float(salary)\n",
        "    else:\n",
        "        salary_val = feature_info['salary_mean']\n",
        "    salary_norm = salary_val / feature_info['salary_max']\n",
        "    print(f\"Normalized salary: {salary_norm:.3f}\")\n",
        "\n",
        "    # Experience\n",
        "    if exp and exp.strip():\n",
        "        exp_val = float(exp)\n",
        "    else:\n",
        "        exp_val = feature_info['exp_mean']\n",
        "    exp_norm = exp_val / feature_info['exp_max']\n",
        "    print(f\"Normalized experience: {exp_norm:.3f}\")\n",
        "\n",
        "    # Location (one-hot for top 10)\n",
        "    loc_clean = location.lower().strip()\n",
        "    loc_vec = np.array([1 if loc_clean == loc else 0 for loc in feature_info['top_locations']]).reshape(1, -1)\n",
        "    print(f\"Location vector shape: {loc_vec.shape}\")\n",
        "\n",
        "    # Combine features\n",
        "    user_features = np.hstack((skills_vec, [[salary_norm, exp_norm]], loc_vec))\n",
        "    print(f\"Combined user features shape: {user_features.shape}\")\n",
        "\n",
        "    # Reduce with SVD\n",
        "    user_features_reduced = svd.transform(user_features)\n",
        "    print(f\"Reduced user features shape: {user_features_reduced.shape}\")\n",
        "    \n",
        "    return user_features_reduced\n",
        "\n",
        "# Get recommendations based on user input\n",
        "def get_recommendations(user_input, df, kmeans, job_features_reduced, num_recommendations=5):\n",
        "    print(\"Getting recommendations...\")\n",
        "    \n",
        "    # Find user's cluster\n",
        "    user_cluster = kmeans.predict(user_input)[0]\n",
        "    print(f\"User assigned to cluster: {user_cluster}\")\n",
        "    \n",
        "    # Filter jobs in user's cluster\n",
        "    cluster_jobs_idx = df[df[\"cluster\"] == user_cluster].index\n",
        "    print(f\"Number of jobs in cluster {user_cluster}: {len(cluster_jobs_idx)}\")\n",
        "    \n",
        "    cluster_job_features = job_features_reduced[cluster_jobs_idx]\n",
        "    \n",
        "    # Use kNN to find top recommendations\n",
        "    knn = NearestNeighbors(n_neighbors=num_recommendations, metric=\"cosine\")\n",
        "    knn.fit(cluster_job_features)\n",
        "    distances, indices = knn.kneighbors(user_input)\n",
        "    \n",
        "    # Get recommended jobs\n",
        "    recommended_indices = cluster_jobs_idx[indices[0]]\n",
        "    recommendations = df.iloc[recommended_indices][[\"Title\", \"Company\", \"Salary\", \"Location\", \"Experience\", \"job_link\"]]\n",
        "    \n",
        "    # Calculate similarity score (1 - distance)\n",
        "    similarities = 1 - distances[0]\n",
        "    recommendations['similarity'] = similarities\n",
        "    \n",
        "    print(f\"Average similarity score: {similarities.mean():.3f}\")\n",
        "    \n",
        "    return recommendations, user_cluster, similarities.mean()\n",
        "\n",
        "# Main function to perform the entire recommendation process\n",
        "def recommend_jobs(skills, salary, exp, location, num_recommendations=5):\n",
        "    # Load all necessary models and data\n",
        "    df, vectorizer, svd, kmeans, feature_info, job_features_reduced = load_models()\n",
        "    \n",
        "    # Preprocess user input\n",
        "    user_input = preprocess_user_input(skills, salary, exp, location, vectorizer, svd, feature_info)\n",
        "    \n",
        "    # Get recommendations\n",
        "    recommendations, user_cluster, avg_similarity = get_recommendations(\n",
        "        user_input, df, kmeans, job_features_reduced, num_recommendations\n",
        "    )\n",
        "    \n",
        "    return recommendations, user_cluster, avg_similarity\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample user input\n",
        "    sample_skills = \"python, java , pyspark , \"\n",
        "    sample_salary = \"2\"  # LPA\n",
        "    sample_exp = \"0\"      # years\n",
        "    sample_location = \"noida\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Getting recommendations for:\")\n",
        "    print(f\"Skills: {sample_skills}\")\n",
        "    print(f\"Salary: {sample_salary} LPA\")\n",
        "    print(f\"Experience: {sample_exp} years\")\n",
        "    print(f\"Location: {sample_location}\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "    \n",
        "    # Get recommendations\n",
        "    recommendations, user_cluster, avg_similarity = recommend_jobs(\n",
        "        sample_skills, sample_salary, sample_exp, sample_location\n",
        "    )\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Recommendation Results:\")\n",
        "    print(f\"User assigned to cluster: {user_cluster}\")\n",
        "    print(f\"Average similarity score: {avg_similarity:.3f} ({avg_similarity*100:.1f}%)\")\n",
        "    print(f\"Top {len(recommendations)} recommendations:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    for i, (_, job) in enumerate(recommendations.iterrows(), 1):\n",
        "        print(f\"\\nRecommendation #{i} (Similarity: {job['similarity']*100:.1f}%):\")\n",
        "        print(f\"Title: {job['Title']}\")\n",
        "        print(f\"Company: {job['Company']}\")\n",
        "        print(f\"Salary: {job['Salary']}\")\n",
        "        print(f\"Location: {job['Location']}\")\n",
        "        print(f\"Experience: {job['Experience']}\")\n",
        "        print(f\"Job Link: {job['job_link']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
